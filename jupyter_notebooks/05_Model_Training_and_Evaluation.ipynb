{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84ca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c096a4",
   "metadata": {},
   "source": [
    "# 05 ‚Äì Model Training & Evaluation  \n",
    "**CRISP-DM Phase 4: Modeling** (and Phase 5: Evaluation)  \n",
    "This notebook fits a regression model, evaluates its performance, and serialises artifacts for deployment.\n",
    "\n",
    "### Objectives\n",
    "* One-hot encode features to match model expectations.  \n",
    "* Split data 80 / 20 (with `random_state=42`).  \n",
    "* Fit a baseline `LinearRegression` model.  \n",
    "* Evaluate on test set: R¬≤, MAE, RMSE.  \n",
    "* Serialize artifacts for deployment:  \n",
    "  - `house_price_model.pkl`  \n",
    "  - `model_columns.pkl`  \n",
    "  - *(optional)* `model_metrics.json`  \n",
    "\n",
    "### Inputs\n",
    "* `outputs/datasets/collection/HousePricesRecords_clean.csv`  \n",
    "\n",
    "### Outputs\n",
    "* `outputs/models/house_price_model.pkl`  \n",
    "* `outputs/models/model_columns.pkl`  \n",
    "* *(optional)* `outputs/models/model_metrics.json`  \n",
    "\n",
    "### Additional Comments  \n",
    "#### Business Requirements Addressed  \n",
    "* **BR3**: Produces the trained model for the Sale Price Prediction tab.  \n",
    "\n",
    "#### Additional Notes  \n",
    "* Later: upgrade to a pipeline with XGBoost + hyperparameter tuning to boost performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed9ab2",
   "metadata": {},
   "source": [
    "### Import Required Libraries for Modeling & Evaluation  \n",
    "This cell brings in the modules we‚Äôll need to load data and the trained model, split the dataset, fit our regression algorithm, and compute performance metrics:\n",
    "\n",
    "- **`os`** for file‚Äêsystem operations (ensuring output folders exist, constructing paths).  \n",
    "- **`joblib`** to deserialize the previously saved `house_price_model.pkl` and `model_columns.pkl`.  \n",
    "- **`pandas as pd`** for tabular data manipulation (loading CSV, creating DataFrames).  \n",
    "- **`train_test_split`** and **`LinearRegression`** from **`sklearn`** for splitting data and fitting the baseline regression model.  \n",
    "- **`r2_score`** and **`mean_absolute_error`** for evaluating model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f11d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90a114",
   "metadata": {},
   "source": [
    "### Load cleaned data\n",
    "\n",
    "y  ‚Üí target variable (what we want to predict)\n",
    "\n",
    "X  ‚Üí feature matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9777681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/datasets/collection/HousePricesRecords_clean.csv\")\n",
    "y  = df[\"Price\"]\n",
    "X  = df.drop(columns=[\"Price\", \"Date of Transfer\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3de88",
   "metadata": {},
   "source": [
    "#### Define numeric & categorical feature lists\n",
    "\n",
    "Numeric features (continuous variables to be scaled with a `StandardScaler`)  \n",
    "\n",
    "Categorical features (discrete variables to be one-hot-encoded with a `OneHotEncoder`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe4dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"Year\", \"Month\",\n",
    "    \"RegionMedianPrice\", \"RegionSaleCount\",\n",
    "    \"CountyMedianPrice\", \"CountySaleCount\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Old/New\", \"Duration\",\n",
    "    \"Town/City\", \"County\", \"PPDCategory Type\",\n",
    "    \"Property_D\", \"Property_F\", \"Property_S\", \"Property_T\",\n",
    "    \"Region\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0305fa6",
   "metadata": {},
   "source": [
    "#### Filter Feature Lists to Available Columns\n",
    "\n",
    "This code ensures that both your numeric and categorical feature lists only include columns that actually exist in the current DataFrame `X`.  \n",
    "This prevents errors in your preprocessing pipeline if any expected column is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bdb8e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features     = [c for c in numeric_features     if c in X.columns]\n",
    "categorical_features = [c for c in categorical_features if c in X.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad345a63",
   "metadata": {},
   "source": [
    "#### Build Preprocessing & Modeling Pipeline  \n",
    "This block creates a scikit-learn `ColumnTransformer` named `preprocessor` that:\n",
    "\n",
    "Scales all numeric features using `StandardScaler` (zero mean, unit variance).  One-hot encodes all categorical features (with unseen categories ignored).\n",
    "\n",
    "It then defines a `Pipeline` that sequentially:\n",
    "\n",
    "Applies the `preprocessor` to prepare the data Fits a `RandomForestRegressor` (named ‚Äúregressor‚Äù) on the transformed features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e1c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), numeric_features),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45fac08",
   "metadata": {},
   "source": [
    "#### Split Data into Training and Test Sets  \n",
    "This cell uses scikit-learn‚Äôs `train_test_split` to randomly split our feature matrix `X` and target vector `y` into:\n",
    "\n",
    "- **Training set** (`X_train`, `y_train`) comprising 80 % of the data, used to fit the model.  \n",
    "- **Test set** (`X_test`, `y_test`) comprising 20 % of the data, reserved for evaluating performance on unseen examples.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83477932",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,  \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4e38c",
   "metadata": {},
   "source": [
    "#### Hyperparameter Grid Search with Cross-Validation\n",
    "\n",
    "this cell define a `param_grid` dictionary listing different settings to try for a Random Forest regressor‚Äîsuch as the number of trees, tree depth, and how splits are made. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a4e48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"regressor__n_estimators\":      [100, 200, 300],\n",
    "    \"regressor__max_depth\":         [5, 10, 20],\n",
    "    \"regressor__min_samples_split\": [2, 5, 10],\n",
    "    \"regressor__min_samples_leaf\":  [1, 2, 4],\n",
    "    \"regressor__max_features\":      [\"sqrt\", \"log2\", None],\n",
    "    \"regressor__bootstrap\":         [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc7045",
   "metadata": {},
   "source": [
    "#### RandomizedSearchCV\n",
    "Samples 50 random hyper-parameter settings (out of the full grid), runs 3-fold CV on each, and picks the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d25b7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e029a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "üîç Best params (random): {'regressor__n_estimators': 100, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 4, 'regressor__max_features': None, 'regressor__max_depth': 5, 'regressor__bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"üîç Best params (random):\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6df5b",
   "metadata": {},
   "source": [
    "#### Check for Any Remaining Non-Numeric Features  \n",
    "This cell verifies that our feature matrix `X` contains only numeric columns after one-hot encoding. It uses `select_dtypes(exclude=[np.number])` to list any columns that still aren‚Äôt numeric. An empty list means you‚Äôre safe to proceed; if any names appear, you‚Äôll need to encode or drop those fields before fitting the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73efc165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still non-numeric: ['Town/City', 'County', 'PPDCategory Type', 'Property_D', 'Property_F', 'Property_O', 'Property_S', 'Property_T', 'Region']\n"
     ]
    }
   ],
   "source": [
    "non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"Still non-numeric:\", non_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb05a8f",
   "metadata": {},
   "source": [
    "#### List Available Training Features  \n",
    "This cell prints out all column names in `X_train` so you can verify which features are present after splitting. It‚Äôs a quick check to ensure your preprocessing and feature lists align with the actual training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2915e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Old/New', 'Duration', 'Town/City', 'County', 'PPDCategory Type', 'Year', 'Month', 'Property_D', 'Property_F', 'Property_O', 'Property_S', 'Property_T', 'Region', 'RegionMedianPrice', 'RegionSaleCount', 'CountyMedianPrice', 'CountySaleCount', 'LogPrice']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available columns:\", X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da25fb",
   "metadata": {},
   "source": [
    "#### Evaluate Model Performance on Test Set  \n",
    "This cell uses the tuned `best_model` to predict sale prices for the held-out `X_test` data, then reports the Mean Absolute Error (MAE) and R¬≤ score to show how accurately the model generalises to new, unseen properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da4684c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test MAE: 69666.93602448335\n",
      "üìà Test R¬≤ : 0.52982827718276\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(\"üìä Test MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"üìà Test R¬≤ :\", r2_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe5bd2",
   "metadata": {},
   "source": [
    "#### Save the Trained Pipeline  \n",
    "This cell ensures the `outputs/models` folder exists, serialises the tuned `best_model` pipeline to `house_price_pipeline.pkl` using `joblib.dump`, and prints a confirmation so you can load it later for live predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "330ae066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline saved to ../outputs/models/house_price_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"../outputs/models\", exist_ok=True)\n",
    "joblib.dump(best_model, \"../outputs/models/house_price_pipeline.pkl\")\n",
    "print(\"‚úÖ Pipeline saved to ../outputs/models/house_price_pipeline.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
