{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84ca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c096a4",
   "metadata": {},
   "source": [
    "# 05 – Model Training & Evaluation  \n",
    "**CRISP-DM Phase 4: Modeling** (and Phase 5: Evaluation)  \n",
    "This notebook fits a regression model, evaluates its performance, and serialises artifacts for deployment.\n",
    "\n",
    "### Objectives\n",
    "* One-hot encode features to match model expectations.  \n",
    "* Split data 80 / 20 (with `random_state=42`).  \n",
    "* Fit a baseline `LinearRegression` model.  \n",
    "* Evaluate on test set: R², MAE, RMSE.  \n",
    "* Serialize artifacts for deployment:  \n",
    "  - `house_price_model.pkl`  \n",
    "  - `model_columns.pkl`  \n",
    "  - *(optional)* `model_metrics.json`  \n",
    "\n",
    "### Inputs\n",
    "* `outputs/datasets/collection/HousePricesRecords_clean.csv`  \n",
    "\n",
    "### Outputs\n",
    "* `outputs/models/house_price_model.pkl`  \n",
    "* `outputs/models/model_columns.pkl`  \n",
    "* *(optional)* `outputs/models/model_metrics.json`  \n",
    "\n",
    "### Additional Comments  \n",
    "#### Business Requirements Addressed  \n",
    "* **BR3**: Produces the trained model for the Sale Price Prediction tab.  \n",
    "\n",
    "#### Additional Notes  \n",
    "* Later: upgrade to a pipeline with XGBoost + hyperparameter tuning to boost performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed9ab2",
   "metadata": {},
   "source": [
    "### Import Required Libraries for Modeling & Evaluation  \n",
    "This cell brings in the modules we’ll need to load data and the trained model, split the dataset, fit our regression algorithm, and compute performance metrics:\n",
    "\n",
    "- **`os`** for file‐system operations (ensuring output folders exist, constructing paths).  \n",
    "- **`joblib`** to deserialize the previously saved `house_price_model.pkl` and `model_columns.pkl`.  \n",
    "- **`pandas as pd`** for tabular data manipulation (loading CSV, creating DataFrames).  \n",
    "- **`train_test_split`** and **`LinearRegression`** from **`sklearn`** for splitting data and fitting the baseline regression model.  \n",
    "- **`r2_score`** and **`mean_absolute_error`** for evaluating model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f11d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90a114",
   "metadata": {},
   "source": [
    "### Load cleaned data\n",
    "\n",
    "y  → target variable (what we want to predict)\n",
    "\n",
    "X  → feature matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9777681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/datasets/collection/HousePricesRecords_clean.csv\")\n",
    "y  = df[\"Price\"]\n",
    "X  = df.drop(columns=[\"Price\", \"Date of Transfer\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3de88",
   "metadata": {},
   "source": [
    "#### Define numeric & categorical feature lists\n",
    "\n",
    "Numeric features (continuous variables to be scaled with a `StandardScaler`)  \n",
    "\n",
    "Categorical features (discrete variables to be one-hot-encoded with a `OneHotEncoder`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe4dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"Year\", \"Month\",\n",
    "    \"RegionMedianPrice\", \"RegionSaleCount\",\n",
    "    \"CountyMedianPrice\", \"CountySaleCount\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Old/New\", \"Duration\",\n",
    "    \"Town/City\", \"County\", \"PPDCategory Type\",\n",
    "    \"Property_D\", \"Property_F\", \"Property_S\", \"Property_T\",\n",
    "    \"Region\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0305fa6",
   "metadata": {},
   "source": [
    "#### Filter Feature Lists to Available Columns\n",
    "\n",
    "This code ensures that both your numeric and categorical feature lists only include columns that actually exist in the current DataFrame `X`.  \n",
    "This prevents errors in your preprocessing pipeline if any expected column is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdb8e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features     = [c for c in numeric_features     if c in X.columns]\n",
    "categorical_features = [c for c in categorical_features if c in X.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad345a63",
   "metadata": {},
   "source": [
    "#### Build Preprocessing & Modeling Pipeline  \n",
    "This block creates a scikit-learn `ColumnTransformer` named `preprocessor` that:\n",
    "\n",
    "Scales all numeric features using `StandardScaler` (zero mean, unit variance).  One-hot encodes all categorical features (with unseen categories ignored).\n",
    "\n",
    "It then defines a `Pipeline` that sequentially:\n",
    "\n",
    "Applies the `preprocessor` to prepare the data Fits a `RandomForestRegressor` (named “regressor”) on the transformed features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e1c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45fac08",
   "metadata": {},
   "source": [
    "#### Split Data into Training and Test Sets  \n",
    "This cell uses scikit-learn’s `train_test_split` to randomly split our feature matrix `X` and target vector `y` into:\n",
    "\n",
    "- **Training set** (`X_train`, `y_train`) comprising 80 % of the data, used to fit the model.  \n",
    "- **Test set** (`X_test`, `y_test`) comprising 20 % of the data, reserved for evaluating performance on unseen examples.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83477932",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,  \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4e38c",
   "metadata": {},
   "source": [
    "#### Hyperparameter Grid Search with Cross-Validation\n",
    "\n",
    "this cell define a `param_grid` dictionary listing different settings to try for a Random Forest regressor—such as the number of trees, tree depth, and how splits are made. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4e48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"regressor__n_estimators\":      [100, 200, 300],\n",
    "    \"regressor__max_depth\":         [5, 10, 20],\n",
    "    \"regressor__min_samples_split\": [2, 5, 10],\n",
    "    \"regressor__min_samples_leaf\":  [1, 2, 4],\n",
    "    \"regressor__max_features\":      [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"regressor__bootstrap\":         [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc7045",
   "metadata": {},
   "source": [
    "then wrap our entire preprocessing-and-model pipeline in a `GridSearchCV` object, asking it to evaluate each combination of these settings using 5-fold cross-validation and to optimise for mean absolute error. When we call `grid_search.fit(X_train, y_train)`, it trains and scores over 2,400 candidate models across different train/validation splits to find the configuration that yields the lowest error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98c21e",
   "metadata": {},
   "source": [
    "After the search finishes, `grid_search.best_estimator_` holds the pipeline with the optimal hyperparameters. We save that as `best_model` and print out `grid_search.best_params_` so we know exactly which parameter values produced the best results—ready for final evaluation on the test set or for deployment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6df5b",
   "metadata": {},
   "source": [
    "#### Check for Any Remaining Non-Numeric Features  \n",
    "This cell verifies that our feature matrix `X` contains only numeric columns after one-hot encoding. It uses `select_dtypes(exclude=[np.number])` to list any columns that still aren’t numeric. An empty list means you’re safe to proceed; if any names appear, you’ll need to encode or drop those fields before fitting the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73efc165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still non-numeric: ['Town/City', 'County', 'PPDCategory Type', 'Property_D', 'Property_F', 'Property_S', 'Property_T', 'Region']\n"
     ]
    }
   ],
   "source": [
    "non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"Still non-numeric:\", non_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb05a8f",
   "metadata": {},
   "source": [
    "#### List Available Training Features  \n",
    "This cell prints out all column names in `X_train` so you can verify which features are present after splitting. It’s a quick check to ensure your preprocessing and feature lists align with the actual training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2915e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Old/New', 'Duration', 'Town/City', 'County', 'PPDCategory Type', 'Year', 'Month', 'Property_D', 'Property_F', 'Property_S', 'Property_T', 'Region', 'RegionMedianPrice', 'RegionSaleCount', 'CountyMedianPrice', 'CountySaleCount', 'LogPrice']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available columns:\", X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da25fb",
   "metadata": {},
   "source": [
    "#### Evaluate Model Performance on Test Set  \n",
    "This cell uses the tuned `best_model` to predict sale prices for the held-out `X_test` data, then reports the Mean Absolute Error (MAE) and R² score to show how accurately the model generalises to new, unseen properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4684c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Test MAE: 20858.91501050527\n",
      "📈 Test R² : 0.4696840577457815\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(\"📊 Test MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"📈 Test R² :\", r2_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe5bd2",
   "metadata": {},
   "source": [
    "#### Save the Trained Pipeline  \n",
    "This cell ensures the `outputs/models` folder exists, serialises the tuned `best_model` pipeline to `house_price_pipeline.pkl` using `joblib.dump`, and prints a confirmation so you can load it later for live predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ae066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline saved to ../outputs/models/house_price_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"../outputs/models\", exist_ok=True)\n",
    "joblib.dump(best_model, \"../outputs/models/house_price_pipeline.pkl\")\n",
    "print(\"✅ Pipeline saved to ../outputs/models/house_price_pipeline.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
