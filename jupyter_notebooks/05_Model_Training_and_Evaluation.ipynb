{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84ca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c096a4",
   "metadata": {},
   "source": [
    "# 05 – Model Training & Evaluation  \n",
    "**CRISP-DM Phase 4: Modeling** (and Phase 5: Evaluation)  \n",
    "This notebook fits a regression model, evaluates its performance, and serialises artifacts for deployment.\n",
    "\n",
    "### Objectives\n",
    "* One-hot encode features to match model expectations.  \n",
    "* Split data 80 / 20 (with `random_state=42`).  \n",
    "* Fit a baseline `LinearRegression` model.  \n",
    "* Evaluate on test set: R², MAE, RMSE.  \n",
    "* Serialize artifacts for deployment:  \n",
    "  - `house_price_model.pkl`  \n",
    "  - `model_columns.pkl`  \n",
    "  - *(optional)* `model_metrics.json`  \n",
    "\n",
    "### Inputs\n",
    "* `outputs/datasets/collection/HousePricesRecords_clean.csv`  \n",
    "\n",
    "### Outputs\n",
    "* `outputs/models/house_price_model.pkl`  \n",
    "* `outputs/models/model_columns.pkl`  \n",
    "* *(optional)* `outputs/models/model_metrics.json`  \n",
    "\n",
    "### Additional Comments  \n",
    "#### Business Requirements Addressed  \n",
    "* **BR3**: Produces the trained model for the Sale Price Prediction tab.  \n",
    "\n",
    "#### Additional Notes  \n",
    "* Later: upgrade to a pipeline with XGBoost + hyperparameter tuning to boost performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed9ab2",
   "metadata": {},
   "source": [
    "### Import Required Libraries for Modeling & Evaluation  \n",
    "This cell brings in the modules we’ll need to load data and the trained model, split the dataset, fit our regression algorithm, and compute performance metrics:\n",
    "\n",
    "- **`os`** for file‐system operations (ensuring output folders exist, constructing paths).  \n",
    "- **`joblib`** to deserialize the previously saved `house_price_model.pkl` and `model_columns.pkl`.  \n",
    "- **`pandas as pd`** for tabular data manipulation (loading CSV, creating DataFrames).  \n",
    "- **`train_test_split`** and **`LinearRegression`** from **`sklearn`** for splitting data and fitting the baseline regression model.  \n",
    "- **`r2_score`** and **`mean_absolute_error`** for evaluating model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f11d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90a114",
   "metadata": {},
   "source": [
    "### Load cleaned data\n",
    "\n",
    "y  → target variable (what we want to predict)\n",
    "\n",
    "X  → feature matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9777681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/datasets/collection/HousePricesRecords_clean.csv\")\n",
    "y  = df[\"Price\"]\n",
    "X  = df.drop(columns=[\"Price\", \"Date of Transfer\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3de88",
   "metadata": {},
   "source": [
    "#### Define numeric & categorical feature lists\n",
    "\n",
    "Numeric features (continuous variables to be scaled with a `StandardScaler`)  \n",
    "\n",
    "Categorical features (discrete variables to be one-hot-encoded with a `OneHotEncoder`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe4dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"Year\", \"Month\",\n",
    "    \"RegionMedianPrice\", \"RegionSaleCount\",\n",
    "    \"CountyMedianPrice\", \"CountySaleCount\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Old/New\", \"Duration\",\n",
    "    \"Town/City\", \"County\", \"PPDCategory Type\",\n",
    "    \"Property_D\", \"Property_F\", \"Property_S\", \"Property_T\",\n",
    "    \"Region\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0305fa6",
   "metadata": {},
   "source": [
    "#### Filter Feature Lists to Available Columns\n",
    "\n",
    "This code ensures that both your numeric and categorical feature lists only include columns that actually exist in the current DataFrame `X`.  \n",
    "This prevents errors in your preprocessing pipeline if any expected column is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdb8e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features     = [c for c in numeric_features     if c in X.columns]\n",
    "categorical_features = [c for c in categorical_features if c in X.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad345a63",
   "metadata": {},
   "source": [
    "#### Build Preprocessing & Modeling Pipeline  \n",
    "This block creates a scikit-learn `ColumnTransformer` named `preprocessor` that:\n",
    "\n",
    "Scales all numeric features using `StandardScaler` (zero mean, unit variance).  One-hot encodes all categorical features (with unseen categories ignored).\n",
    "\n",
    "It then defines a `Pipeline` that sequentially:\n",
    "\n",
    "Applies the `preprocessor` to prepare the data Fits a `RandomForestRegressor` (named “regressor”) on the transformed features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e1c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45fac08",
   "metadata": {},
   "source": [
    "#### Split Data into Training and Test Sets  \n",
    "This cell uses scikit-learn’s `train_test_split` to randomly split our feature matrix `X` and target vector `y` into:\n",
    "\n",
    "- **Training set** (`X_train`, `y_train`) comprising 80 % of the data, used to fit the model.  \n",
    "- **Test set** (`X_test`, `y_test`) comprising 20 % of the data, reserved for evaluating performance on unseen examples.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83477932",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,  \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4e38c",
   "metadata": {},
   "source": [
    "#### Hyperparameter Grid Search with Cross-Validation\n",
    "\n",
    "this cell define a `param_grid` dictionary listing different settings to try for a Random Forest regressor—such as the number of trees, tree depth, and how splits are made. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4e48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"regressor__n_estimators\":      [100, 200, 300],\n",
    "    \"regressor__max_depth\":         [5, 10, 20],\n",
    "    \"regressor__min_samples_split\": [2, 5, 10],\n",
    "    \"regressor__min_samples_leaf\":  [1, 2, 4],\n",
    "    \"regressor__max_features\":      [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"regressor__bootstrap\":         [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc7045",
   "metadata": {},
   "source": [
    "then wrap our entire preprocessing-and-model pipeline in a `GridSearchCV` object, asking it to evaluate each combination of these settings using 5-fold cross-validation and to optimise for mean absolute error. When we call `grid_search.fit(X_train, y_train)`, it trains and scores over 2,400 candidate models across different train/validation splits to find the configuration that yields the lowest error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25b7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98c21e",
   "metadata": {},
   "source": [
    "After the search finishes, `grid_search.best_estimator_` holds the pipeline with the optimal hyperparameters. We save that as `best_model` and print out `grid_search.best_params_` so we know exactly which parameter values produced the best results—ready for final evaluation on the test set or for deployment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e029a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cistudent/.local/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "810 fits failed out of a total of 2430.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "429 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "381 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/cistudent/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/cistudent/.local/lib/python3.12/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [            nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan -21811.97062366\n",
      " -21782.30175083 -21783.45047361 -21827.87479273 -21739.40392589\n",
      " -21702.38529929 -21798.38333454 -21720.70875863 -21714.7784659\n",
      " -21745.14245812 -21722.78576011 -21707.4876203  -21791.7821666\n",
      " -21727.37533468 -21705.43972738 -21830.08913988 -21767.0865428\n",
      " -21745.16711956 -22405.38295118 -22438.54163317 -22360.41211114\n",
      " -22405.38295118 -22438.54163317 -22360.41211114 -22355.33876258\n",
      " -22418.30286101 -22346.52362597 -25019.65171997 -24856.82179905\n",
      " -24840.76793878 -25033.25899278 -24898.59199427 -24871.10594495\n",
      " -25033.56494542 -24901.59408371 -24877.36077213 -25381.63790292\n",
      " -25214.68248997 -25214.75971724 -25396.81542766 -25224.84184197\n",
      " -25220.26356096 -25373.99240275 -25208.21993092 -25210.57058497\n",
      " -26414.33937536 -26321.1961011  -26261.17779007 -26414.33937536\n",
      " -26321.1961011  -26261.17779007 -26427.55636554 -26328.27796973\n",
      " -26262.44511112             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      " -19298.91859375 -19349.92189592 -19383.96363123 -19305.37869553\n",
      " -19308.20735718 -19317.34981698 -19464.07153944 -19458.39659246\n",
      " -19428.38711861 -19698.63457344 -19675.28359525 -19650.68316032\n",
      " -19670.77612146 -19653.68470703 -19654.54513413 -19734.15091734\n",
      " -19722.96371051 -19684.66788364 -21399.35073837 -21417.6609145\n",
      " -21393.10673201 -21399.35073837 -21417.6609145  -21393.10673201\n",
      " -21371.04207098 -21401.14821649 -21398.32111111 -23001.17151189\n",
      " -22857.37236308 -22836.33774343 -23054.19828156 -22886.02661602\n",
      " -22869.56657899 -23027.90355492 -22838.2838271  -22837.08081511\n",
      " -24398.51281993 -24106.63591022 -24115.48642263 -24389.88346074\n",
      " -24101.66449296 -24103.90527127 -24437.1786325  -24132.13383345\n",
      " -24147.97201408 -26378.75476587 -26284.75095065 -26223.65938673\n",
      " -26378.75476587 -26284.75095065 -26223.65938673 -26389.25370158\n",
      " -26289.13000839 -26222.8793909              nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan -18326.82196018 -18310.7112316  -18288.88016425\n",
      " -17913.96287185 -18002.50042213 -18051.94138354 -17913.56989889\n",
      " -17925.89577161 -17947.71809605 -18840.78505351 -18866.08574281\n",
      " -18811.39616422 -18735.93899222 -18816.68926413 -18806.42574109\n",
      " -18747.30463684 -18795.38492463 -18794.76787661 -21245.38567893\n",
      " -21296.60989617 -21288.58685985 -21245.38567893 -21296.60989617\n",
      " -21288.58685985 -21226.22499286 -21305.94008561 -21303.44631501\n",
      " -20878.5240403  -20704.53315974 -20617.40588801 -20812.52842078\n",
      " -20638.86102659 -20573.41542806 -20821.3509509  -20744.16004498\n",
      " -20624.71326182 -23727.34647574 -23509.79522981 -23533.71050749\n",
      " -23769.46691368 -23530.88000457 -23538.91398609 -23816.98575806\n",
      " -23564.10627461 -23582.71175144 -26378.75476587 -26283.48181069\n",
      " -26222.81329343 -26378.75476587 -26283.48181069 -26222.81329343\n",
      " -26389.25370158 -26286.7303247  -26221.27960177             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan -21844.2933942  -21799.09324419\n",
      " -21824.1365406  -21865.70521931 -21839.2345359  -21836.27971111\n",
      " -21848.36999926 -21816.76484079 -21814.73535962 -21830.98418918\n",
      " -21784.82213656 -21724.52118926 -21784.0420404  -21784.93771774\n",
      " -21720.53594768 -21771.62133357 -21764.87507626 -21707.92599457\n",
      " -22022.48060544 -21910.26865517 -21911.31785084 -22022.48060544\n",
      " -21910.26865517 -21911.31785084 -21994.4509361  -21909.90192133\n",
      " -21907.45689389 -24877.182001   -24857.8380245  -24787.82207984\n",
      " -24891.88246047 -24874.41327328 -24798.88003074 -24882.94951016\n",
      " -24861.05590869 -24798.24683349 -24903.78724902 -24916.32480937\n",
      " -24857.18994647 -24905.05106194 -24929.09115075 -24863.48418203\n",
      " -24887.43729898 -24914.8102048  -24856.29933247 -25721.66985327\n",
      " -25714.72656415 -25625.2749463  -25721.66985327 -25714.72656415\n",
      " -25625.2749463  -25723.8072037  -25701.76031764 -25621.45201079\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan -19497.40107\n",
      " -19441.8170762  -19478.59869794 -19572.66319213 -19498.37447633\n",
      " -19502.14296888 -19417.18614501 -19391.66511042 -19389.91071402\n",
      " -19494.97491918 -19460.61597596 -19489.64340407 -19542.36376057\n",
      " -19455.01961161 -19448.80600332 -19583.23655482 -19440.06483574\n",
      " -19422.52986627 -20454.31079599 -20332.22351771 -20310.79661637\n",
      " -20454.31079599 -20332.22351771 -20310.79661637 -20441.08997866\n",
      " -20315.33996253 -20302.88232284 -23040.19680455 -22950.30294423\n",
      " -22837.68825708 -23109.87462741 -23008.65175464 -22900.8183969\n",
      " -23073.2937645  -22959.94787862 -22882.75206196 -23308.66627511\n",
      " -23214.53409853 -23135.47296771 -23345.69432564 -23249.29867532\n",
      " -23168.24119347 -23363.77523824 -23274.46477722 -23215.56138322\n",
      " -25559.8270548  -25481.15266901 -25386.91324362 -25559.8270548\n",
      " -25481.15266901 -25386.91324362 -25567.05528647 -25470.58861848\n",
      " -25385.2658163              nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      " -18698.92447976 -18658.72662081 -18657.3601035  -18511.03930985\n",
      " -18474.58479364 -18439.57917875 -18270.08834568 -18262.99662153\n",
      " -18258.81747249 -18644.83885514 -18527.24327484 -18534.48854613\n",
      " -18543.56241574 -18527.90164039 -18536.98497456 -18533.22704008\n",
      " -18459.61671725 -18451.73604455 -20070.06170692 -19991.44295206\n",
      " -19969.02498176 -20070.06170692 -19991.44295206 -19969.02498176\n",
      " -20037.09682133 -20007.03845636 -20001.41882139 -20765.09787097\n",
      " -20672.53361072 -20662.35575249 -20738.15563573 -20685.39976739\n",
      " -20674.11035418 -20768.85342474 -20652.18279424 -20598.44095082\n",
      " -21875.51228395 -21891.30029107 -21799.45051451 -21931.34648159\n",
      " -21962.65632236 -21852.38118173 -21876.61604046 -21921.26884569\n",
      " -21856.29980814 -25543.32937957 -25456.77177017 -25368.14091126\n",
      " -25543.32937957 -25456.77177017 -25368.14091126 -25548.48799742\n",
      " -25446.64567026 -25367.48700882]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'regressor__bootstrap': True, 'regressor__max_depth': 20, 'regressor__max_features': 'sqrt', 'regressor__min_samples_leaf': 1, 'regressor__min_samples_split': 10, 'regressor__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6df5b",
   "metadata": {},
   "source": [
    "#### Check for Any Remaining Non-Numeric Features  \n",
    "This cell verifies that our feature matrix `X` contains only numeric columns after one-hot encoding. It uses `select_dtypes(exclude=[np.number])` to list any columns that still aren’t numeric. An empty list means you’re safe to proceed; if any names appear, you’ll need to encode or drop those fields before fitting the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73efc165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still non-numeric: ['Town/City', 'County', 'PPDCategory Type', 'Property_D', 'Property_F', 'Property_S', 'Property_T', 'Region']\n"
     ]
    }
   ],
   "source": [
    "non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"Still non-numeric:\", non_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb05a8f",
   "metadata": {},
   "source": [
    "#### List Available Training Features  \n",
    "This cell prints out all column names in `X_train` so you can verify which features are present after splitting. It’s a quick check to ensure your preprocessing and feature lists align with the actual training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2915e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Old/New', 'Duration', 'Town/City', 'County', 'PPDCategory Type', 'Year', 'Month', 'Property_D', 'Property_F', 'Property_S', 'Property_T', 'Region', 'RegionMedianPrice', 'RegionSaleCount', 'CountyMedianPrice', 'CountySaleCount', 'LogPrice']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available columns:\", X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da25fb",
   "metadata": {},
   "source": [
    "#### Evaluate Model Performance on Test Set  \n",
    "This cell uses the tuned `best_model` to predict sale prices for the held-out `X_test` data, then reports the Mean Absolute Error (MAE) and R² score to show how accurately the model generalises to new, unseen properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da4684c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Test MAE: 20858.91501050527\n",
      "📈 Test R² : 0.4696840577457815\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(\"📊 Test MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"📈 Test R² :\", r2_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe5bd2",
   "metadata": {},
   "source": [
    "#### Save the Trained Pipeline  \n",
    "This cell ensures the `outputs/models` folder exists, serialises the tuned `best_model` pipeline to `house_price_pipeline.pkl` using `joblib.dump`, and prints a confirmation so you can load it later for live predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "330ae066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline saved to ../outputs/models/house_price_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"../outputs/models\", exist_ok=True)\n",
    "joblib.dump(best_model, \"../outputs/models/house_price_pipeline.pkl\")\n",
    "print(\"✅ Pipeline saved to ../outputs/models/house_price_pipeline.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
