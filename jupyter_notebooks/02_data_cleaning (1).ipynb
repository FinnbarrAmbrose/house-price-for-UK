{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549a5fed",
   "metadata": {},
   "source": [
    "# 02 – Data Cleaning & Feature Engineering  \n",
    "**CRISP-DM Phase 3: Data Preparation**  \n",
    "This notebook cleans, standardises and feature-engineers the raw data so it’s ready for modeling and analysis.\n",
    "\n",
    "### Objectives\n",
    "* Load the raw sample CSV from `inputs/datasets/raw/HousePricesRecords.csv`.\n",
    "* Handle nulls and duplicates.\n",
    "* Parse dates → `Year` / `Month`.\n",
    "* Map and one-hot encode categorical fields:  \n",
    "  – `Old/New` → binary flag  \n",
    "  – `Duration` → binary flag (Freehold vs Leasehold)  \n",
    "  – `Property Type` → four flags (`Property_D`, `Property_F`, `Property_S`, `Property_T`)  \n",
    "* Save the fully cleaned, feature-engineered table for downstream use.\n",
    "\n",
    "### Inputs\n",
    "* `inputs/datasets/raw/HousePricesRecords.csv`  \n",
    "\n",
    "### Outputs\n",
    "* `outputs/datasets/collection/HousePricesRecords_clean.csv`  \n",
    "\n",
    "### Additional Comments  \n",
    "#### Business Requirements Addressed  \n",
    "* **BR1**: Ensures high-quality data for the Overview & Correlation pages.  \n",
    "* **BR2**: Creates the features needed for statistical tests on property attributes.  \n",
    "\n",
    "#### Additional Notes  \n",
    "* This cleaned CSV is the single source of truth for Notebooks 3–5 and all Streamlit pages.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fd80d",
   "metadata": {},
   "source": [
    "### Import Required Libraries  \n",
    "This cell brings in the key Python packages we’ll use throughout the data‐cleaning notebook:  \n",
    "- **pandas** (`pd`) for reading, manipulating, and saving tabular data.  \n",
    "- **os** for interacting with the file system (checking/creating folders, constructing file paths).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de9d977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc6eb04",
   "metadata": {},
   "source": [
    "#### Check DataFrame Dimensions  \n",
    "Use `.shape` to quickly see how many rows and columns were loaded into your sample. This confirms you’ve successfully read in 1,000 rows and shows the total number of features available for cleaning and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf61b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../inputs/datasets/raw/price_paid_records.csv\"\n",
    "chunksize = 200_000    # tweak up/down depending on your RAM\n",
    "topn      = 1_000      # how many newest rows to keep\n",
    "latest = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbdcb529",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in pd.read_csv(\n",
    "    input_path,\n",
    "    usecols=[\n",
    "        \"Price\",\"Date of Transfer\",\"Property Type\",\"Old/New\",\n",
    "        \"Duration\",\"Town/City\",\"County\",\"PPDCategory Type\"\n",
    "    ],\n",
    "    parse_dates=[\"Date of Transfer\"],\n",
    "    chunksize=chunksize,\n",
    "    low_memory=False\n",
    "):\n",
    "    # for this chunk, keep its top `topn` newest rows\n",
    "    chunk_top = chunk.nlargest(topn, \"Date of Transfer\")\n",
    "\n",
    "    if latest is None:\n",
    "        latest = chunk_top\n",
    "    else:\n",
    "        # merge with previous bests, then re-take the top `topn`\n",
    "        latest = pd.concat([latest, chunk_top]).nlargest(topn, \"Date of Transfer\")\n",
    "    \n",
    "\n",
    "df_chunk = latest.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f565463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = latest.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c55ec4",
   "metadata": {},
   "source": [
    "Preview the first 5 rows to see if it is working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6e3a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Date of Transfer</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Old/New</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Town/City</th>\n",
       "      <th>County</th>\n",
       "      <th>PPDCategory Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>277000</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>WICKFORD</td>\n",
       "      <td>ESSEX</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30000</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>HULL</td>\n",
       "      <td>CITY OF KINGSTON UPON HULL</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>551000</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>CHISLEHURST</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>240000</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>527500</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>HEMEL HEMPSTEAD</td>\n",
       "      <td>HERTFORDSHIRE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price Date of Transfer Property Type Old/New Duration        Town/City  \\\n",
       "0  277000       2017-06-29             S       N        F         WICKFORD   \n",
       "1   30000       2017-06-29             F       N        L             HULL   \n",
       "2  551000       2017-06-29             T       N        F      CHISLEHURST   \n",
       "3  240000       2017-06-29             S       N        F          BEDFORD   \n",
       "4  527500       2017-06-29             D       N        F  HEMEL HEMPSTEAD   \n",
       "\n",
       "                       County PPDCategory Type  \n",
       "0                       ESSEX                A  \n",
       "1  CITY OF KINGSTON UPON HULL                A  \n",
       "2              GREATER LONDON                A  \n",
       "3                     BEDFORD                B  \n",
       "4               HERTFORDSHIRE                B  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1a84f",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5222e1",
   "metadata": {},
   "source": [
    "#### Inspect Column Data Types  \n",
    "Print out the data type of each column in your sample to verify that numbers, dates, and text fields have been correctly inferred. This helps identify which columns may need conversions (e.g., strings → datetime or numeric types) before cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dad5b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Data Types:\n",
      "Price                        int64\n",
      "Date of Transfer    datetime64[ns]\n",
      "Property Type               object\n",
      "Old/New                     object\n",
      "Duration                    object\n",
      "Town/City                   object\n",
      "County                      object\n",
      "PPDCategory Type            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Column Data Types:\")\n",
    "print(df_chunk.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39560c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ef6a1",
   "metadata": {},
   "source": [
    "#### Examine Category Distributions  \n",
    "This cell loops over key categorical fields to display the unique values and their frequencies. By inspecting `value_counts()`, you can:\n",
    "\n",
    "- Spot typos or inconsistent labels (e.g. “Freehold” vs “freehold”).  \n",
    "- Identify rare categories that may need grouping or special handling.  \n",
    "- Confirm which categories to one-hot encode in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c0c8d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for Property Type:\n",
      "Property Type\n",
      "S    286\n",
      "T    282\n",
      "D    239\n",
      "F    140\n",
      "O     53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for Old/New:\n",
      "Old/New\n",
      "N    992\n",
      "Y      8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for Duration:\n",
      "Duration\n",
      "F    828\n",
      "L    172\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for County:\n",
      "County\n",
      "GREATER LONDON      79\n",
      "KENT                48\n",
      "HAMPSHIRE           39\n",
      "DEVON               39\n",
      "ESSEX               29\n",
      "                    ..\n",
      "BLACKPOOL            1\n",
      "WARRINGTON           1\n",
      "MIDDLESBROUGH        1\n",
      "POWYS                1\n",
      "ISLE OF ANGLESEY     1\n",
      "Name: count, Length: 103, dtype: int64\n",
      "\n",
      "Value counts for Town/City:\n",
      "Town/City\n",
      "LONDON             51\n",
      "BRISTOL            23\n",
      "NOTTINGHAM         15\n",
      "POOLE              12\n",
      "PLYMOUTH           11\n",
      "                   ..\n",
      "KNEBWORTH           1\n",
      "STAMFORD            1\n",
      "BISHOP AUCKLAND     1\n",
      "PENZANCE            1\n",
      "CAERNARFON          1\n",
      "Name: count, Length: 437, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [\"Property Type\", \"Old/New\", \"Duration\", \"County\", \"Town/City\"]\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nValue counts for {col}:\")\n",
    "    print(df_chunk[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46fd423",
   "metadata": {},
   "source": [
    "#### Identify & Remove Rows with Missing Price or Date  \n",
    "This cell first prints the number of null values in each column to highlight data completeness issues. It then drops any rows where **`Price`** or **`Date of Transfer`** is missing—since these fields are critical for analysis—and reports the new DataFrame shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f90d3984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "Price               0\n",
      "Date of Transfer    0\n",
      "Property Type       0\n",
      "Old/New             0\n",
      "Duration            0\n",
      "Town/City           0\n",
      "County              0\n",
      "PPDCategory Type    0\n",
      "dtype: int64\n",
      "Shape after dropping missing price/date: (1000, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Missing values in each column:\")\n",
    "print(df_chunk.isnull().sum())\n",
    "\n",
    "df_chunk.dropna(subset=[\"Price\", \"Date of Transfer\"], inplace=True)\n",
    "print(f\"Shape after dropping missing price/date: {df_chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166683c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ab901",
   "metadata": {},
   "source": [
    "#### Drop Irrelevant Columns  \n",
    "This cell removes columns that aren’t needed for our analysis or modelling, helping to streamline the dataset:\n",
    "\n",
    "- **`Transaction unique identifier`**: a non-informative ID field.  \n",
    "- **`District`**: redundant given the `County` and `Town/City` columns.  \n",
    "- **`Record Status - monthly file only`**: metadata relevant only to monthly file management.  \n",
    "\n",
    "The `errors='ignore'` flag ensures the code won’t fail if any of these columns are already absent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d53db667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk.drop(columns=[\n",
    "    \"Transaction unique identifier\",\n",
    "    \"District\",\n",
    "    \"Record Status - monthly file only\"\n",
    "], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98970b7d",
   "metadata": {},
   "source": [
    "#### Convert “Date of Transfer” to datetime  \n",
    "This cell casts the `Date of Transfer` column from text to pandas `datetime64[ns]`, enabling time-based operations (e.g., extracting year/month, filtering by date).  \n",
    "- `pd.to_datetime(..., errors='coerce')` attempts to parse each entry; any invalid or unrecognisable strings become `NaT` (Not a Time).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e03b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk[\"Date of Transfer\"] = pd.to_datetime(df_chunk[\"Date of Transfer\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190eeac6",
   "metadata": {},
   "source": [
    "#### Extract Year & Month Features  \n",
    "This cell creates two new columns—`Year` and `Month`—by extracting these components from the `Date of Transfer` datetime. These features allow us to analyse and model seasonality or temporal trends in sale prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea33d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk[\"Year\"] = df_chunk[\"Date of Transfer\"].dt.year\n",
    "df_chunk[\"Month\"] = df_chunk[\"Date of Transfer\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8af742b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 rows; remaining 1000 rows from 2014-06-29 → 2017-06-29\n"
     ]
    }
   ],
   "source": [
    "max_date    = df_chunk[\"Date of Transfer\"].max()\n",
    "cutoff_date = max_date - pd.DateOffset(years=3)\n",
    "\n",
    "# 2) filter to only those transfers within the last 3 years\n",
    "before_rows = df_chunk.shape[0]\n",
    "df_chunk    = df_chunk[df_chunk[\"Date of Transfer\"] >= cutoff_date]\n",
    "after_rows  = df_chunk.shape[0]\n",
    "\n",
    "print(\n",
    "    f\"Filtered out {before_rows - after_rows} rows; \"\n",
    "    f\"remaining {after_rows} rows from {cutoff_date.date()} → {max_date.date()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb7d84",
   "metadata": {},
   "source": [
    "#### Encode Key Categorical Features  \n",
    "This cell turns our categorical flags into numeric form so they can be used by the regression model:\n",
    "\n",
    "- **Map binary flags**  \n",
    "  - `Old/New`: maps `'N' → 0` (existing) and `'Y' → 1` (new build)  \n",
    "  - `Duration`: maps `'F' → 1` (freehold) and `'L' → 0` (leasehold)  \n",
    "- **One-hot encode**  \n",
    "  - `Property Type`: creates separate `Property_<Type>` columns (e.g., `Property_Detached`, `Property_Flat`, etc.) with 0/1 indicators  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a9fc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk[\"Old/New\"] = df_chunk[\"Old/New\"].map({'N': 0, 'Y': 1})\n",
    "df_chunk[\"Duration\"] = df_chunk[\"Duration\"].map({'F': 1, 'L': 0})\n",
    "df_chunk = pd.get_dummies(df_chunk, columns=[\"Property Type\"], prefix=\"Property\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fcbcd",
   "metadata": {},
   "source": [
    "#### Remove Duplicate Records & Reset Index  \n",
    "This cell ensures data integrity by dropping any duplicate rows—so each property sale appears only once—and then resets the DataFrame index to a clean, sequential range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b3383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk.drop_duplicates(inplace=True)\n",
    "df_chunk.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d381ae1",
   "metadata": {},
   "source": [
    "### Geographical Aggregates (using Town/City)\n",
    "\n",
    "#### Define a Unified Region Feature  \n",
    "This line creates a new column, `Region`, by copying the values from the existing `Town/City` field.  \n",
    "Having a dedicated `Region` column makes subsequent grouping and aggregation (e.g., median price by region) more intuitive and self-documenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97d29069",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk[\"Region\"] = df_chunk[\"Town/City\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79749085",
   "metadata": {},
   "source": [
    "#### Compute & Merge Regional Median Prices  \n",
    "This block groups the data by `Region`, calculates the median sale price for each region (`RegionMedianPrice`), and then merges that information back into `df_chunk` so each record includes its region’s median price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd21cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_median = (\n",
    "    df_chunk\n",
    "    .groupby(\"Region\")[\"Price\"]\n",
    "    .median()\n",
    "    .rename(\"RegionMedianPrice\")\n",
    ")\n",
    "df_chunk = df_chunk.merge(region_median, on=\"Region\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd9562",
   "metadata": {},
   "source": [
    "#### Compute & Merge Regional Sale Counts  \n",
    "This block counts the number of transactions for each `Region` (`RegionSaleCount`) and then joins that count back into `df_chunk`, so each record knows how many sales occurred in its region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82f30fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_counts = (\n",
    "    df_chunk[\"Region\"]\n",
    "    .value_counts()\n",
    "    .rename(\"RegionSaleCount\")\n",
    ")\n",
    "df_chunk = df_chunk.join(region_counts, on=\"Region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa8610",
   "metadata": {},
   "source": [
    "#### Target Encoding & Count Encoding for County  \n",
    "To capture local market effects without exploding one-hot dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b46ecb",
   "metadata": {},
   "source": [
    "Target-encode the `County` by computing the median sale price per county and merging it in as `CountyMedianPrice`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d074d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_median = (\n",
    "    df_chunk\n",
    "    .groupby(\"County\")[\"Price\"]\n",
    "    .median()\n",
    "    .rename(\"CountyMedianPrice\")\n",
    ")\n",
    "df_chunk = df_chunk.merge(county_median, on=\"County\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b134a8f",
   "metadata": {},
   "source": [
    "Count-encode the `County` by counting how many transactions occurred in each county and merging it in as `CountySaleCount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e56a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_counts = (\n",
    "    df_chunk[\"County\"]\n",
    "    .value_counts()\n",
    "    .rename(\"CountySaleCount\")\n",
    ")\n",
    "df_chunk = df_chunk.join(county_counts, on=\"County\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d5c77",
   "metadata": {},
   "source": [
    "### Price Transformations & Outlier Handling\n",
    "\n",
    "To stabilise variance and reduce the effect of extreme sale prices on our model,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aee8a8",
   "metadata": {},
   "source": [
    "#### Log-transform the target  \n",
    "This step applies a natural log (plus one) to the `Price` column, creating a new `LogPrice` feature. Log transformation helps stabilise variance and reduce right‐skew in the target distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7c00fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk[\"LogPrice\"] = np.log1p(df_chunk[\"Price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421ff2a",
   "metadata": {},
   "source": [
    "####  Compute the 99.5th percentile cutoff  \n",
    "Here we calculate the value at the 99.5th percentile of `LogPrice`. This threshold flags the most extreme high‐price outliers for removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "402080e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_cutoff = df_chunk[\"LogPrice\"].quantile(0.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec7e1f",
   "metadata": {},
   "source": [
    "#### Remove outliers and report  \n",
    "This section filters out any rows where `LogPrice` exceeds the cutoff, then prints how many rows were dropped and the DataFrame’s new shape—ensuring that extreme values don’t unduly influence the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7b74354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 5 outlier rows; new shape: (989, 20)\n"
     ]
    }
   ],
   "source": [
    "before_rows = df_chunk.shape[0]\n",
    "df_chunk = df_chunk[df_chunk[\"LogPrice\"] <= upper_cutoff]\n",
    "after_rows = df_chunk.shape[0]\n",
    "\n",
    "print(f\"Dropped {before_rows - after_rows} outlier rows; new shape: {df_chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c47d8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481af34",
   "metadata": {},
   "source": [
    "#### Save Cleaned Data to CSV  \n",
    "This cell creates the output directory (if it doesn’t already exist), writes the fully cleaned DataFrame to a CSV file, and prints a confirmation message. The resulting file is the single source of truth for downstream analysis and the Streamlit app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49945bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: ../outputs/datasets/collection/HousePricesRecords_clean.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = '../outputs/datasets/collection/HousePricesRecords_clean.csv'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df_chunk.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
